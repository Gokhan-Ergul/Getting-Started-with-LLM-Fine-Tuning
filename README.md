# Getting-Started-with-LLM-Fine-Tuning
This repository demonstrates how to fine-tune a pre-trained transformer model on a GLUE benchmark task using Hugging Face ðŸ¤— Transformers and Datasets.   We use BERT (bert-base-uncased)as the base model and fine-tune it on the MRPC (Microsoft Research Paraphrase Corpus) dataset.  
